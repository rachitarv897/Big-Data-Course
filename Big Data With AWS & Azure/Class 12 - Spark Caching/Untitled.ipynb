{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udccc Managed vs External Tables in Spark\n\nThis notebook explains **Managed vs External Tables in Spark** using an example dataset (`customers_1mb.csv`). It also demonstrates how to configure the **default warehouse path** and verify tables using **Hive Metastore**.\n\n## \ud83d\udd39 Key Concepts\n- **Managed Table**: Spark fully controls the **data and metadata**.\n- **External Table**: Spark manages only **metadata**, while the data remains **outside** Spark's control.\n- **Hive Metastore**: Stores **table definitions** to enable SQL-like querying in Spark.\n\n---"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# \u2705 Step 1: Import necessary libraries\nfrom pyspark.sql import SparkSession\n\n# \u2705 Step 2: Define warehouse directory\nwarehouse_location = '/tmp/mera/warehouse'\n\n# \u2705 Step 3: Create Spark session with Hive support\nspark = SparkSession.builder \\\n    .appName('ManagedVsExternalTables') \\\n    .config('spark.sql.warehouse.dir', warehouse_location) \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\nprint(f\"Spark session created with warehouse location: {warehouse_location}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd0d Checking Current Warehouse Directory\nYou can verify the current **Spark SQL warehouse directory** using the command below."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"data": {"text/plain": "'file:/spark-warehouse'"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "# Show the configured warehouse directory\nspark.conf.get('spark.sql.warehouse.dir')"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcc2 Loading the Dataset\nNow, we load the **customers_1mb.csv** dataset."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Load CSV data into a DataFrame\ndf = spark.read \\\n    .format('csv') \\\n    .option('header', 'True') \\\n    .option('inferSchema', 'True') \\\n    .load('/tmp/customers_1mb.csv')\n\n# Display DataFrame schema\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd04 Creating a Temporary View\nWe'll create a **temporary view** to allow SQL-like queries before creating tables."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n\n"}], "source": "# Create a temporary view for querying\ndf.createOrReplaceTempView(\"temp_customers\")\n\n# Query the view\nspark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83c\udfd7 Creating a **Managed Table**\n- Spark **stores** the data inside the **warehouse directory** (`/tmp/mera/warehouse`).\n- If you **drop** this table, the **data is also deleted**."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n25/02/01 17:20:04 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n25/02/01 17:20:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 Managed table 'managed_customers' created.\n"}], "source": "# Creating a Managed Table\nspark.sql(\"DROP TABLE IF EXISTS managed_customers\")\nspark.sql(\"\"\"\n    CREATE TABLE managed_customers AS \n    SELECT * FROM temp_customers\n\"\"\")\nprint(\"\u2705 Managed table 'managed_customers' created.\")"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/01 17:21:38 WARN HiveExternalCatalog: Request to alter database default is a no-op because the provided database properties are the same as the old ones. Hive does not currently support altering other database fields.\n"}, {"ename": "AnalysisException", "evalue": "Hive 2.3.9 does not support altering database location", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_22272/2718571699.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ALTER DATABASE default SET LOCATION 'file:/tmp/mera/warehouse'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: Hive 2.3.9 does not support altering database location"]}], "source": "spark.sql(\"ALTER DATABASE default SET LOCATION 'file:/tmp/mera/warehouse'\")\n"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+---------------------------------------------------------+-------+\n|col_name                    |data_type                                                |comment|\n+----------------------------+---------------------------------------------------------+-------+\n|customer_id                 |int                                                      |null   |\n|name                        |string                                                   |null   |\n|city                        |string                                                   |null   |\n|state                       |string                                                   |null   |\n|country                     |string                                                   |null   |\n|registration_date           |timestamp                                                |null   |\n|is_active                   |boolean                                                  |null   |\n|                            |                                                         |       |\n|# Detailed Table Information|                                                         |       |\n|Database                    |default                                                  |       |\n|Table                       |managed_customers                                        |       |\n|Owner                       |root                                                     |       |\n|Created Time                |Sat Feb 01 17:20:05 UTC 2025                             |       |\n|Last Access                 |UNKNOWN                                                  |       |\n|Created By                  |Spark 3.3.2                                              |       |\n|Type                        |MANAGED                                                  |       |\n|Provider                    |hive                                                     |       |\n|Table Properties            |[transient_lastDdlTime=1738430407]                       |       |\n|Statistics                  |1219563 bytes                                            |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n+----------------------------+---------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended managed_customers').show(truncate=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd0d Verify Managed Table\nRun the following command in a **Dataproc terminal** to check where the managed table is stored:\n```bash\n!hdfs dfs -ls /tmp/mera/warehouse/managed_customers\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcc2 Creating an **External Table**\n- The data **remains** in `/tmp/customers_1mb.csv`.\n- If you **drop** this table, the **data is not deleted**."}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# Creating an External Table\nspark.sql(\"DROP TABLE IF EXISTS external_customers\")\nspark.sql(\"\"\"\n    CREATE EXTERNAL TABLE external_customers \n    USING CSV \n    LOCATION '/tmp/customers_1mb.csv'\n\"\"\")\nprint(\"\u2705 External table 'external_customers' created.\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd0d Verify External Table\nRun the following command to check its location:\n```bash\n!hdfs dfs -ls /tmp/customers_1mb.csv\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcca Verifying Tables in Hive Metastore\nYou can check all available tables in Spark using:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "# Show tables in Spark\nspark.sql(\"SHOW TABLES\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\uded1 Dropping the Tables\nDropping a **Managed Table** deletes both **metadata and data**, while dropping an **External Table** deletes only **metadata**."}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# Drop managed table (Data is deleted!)\nspark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n\n# Drop external table (Data is NOT deleted!)\nspark.sql(\"DROP TABLE IF EXISTS external_customers\")\n\nprint(\"\u2705 Tables dropped successfully.\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## \ud83c\udfaf Summary\n| Feature | Managed Table | External Table |\n|---------|--------------|----------------|\n| Data Location | Inside warehouse | Custom location |\n| Dropping Table | Deletes data | Only deletes metadata |\n| Performance | Optimized by Spark | Depends on external storage |\n\n**\ud83d\ude80 Now you understand the difference between Managed and External Tables in Spark!**"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}