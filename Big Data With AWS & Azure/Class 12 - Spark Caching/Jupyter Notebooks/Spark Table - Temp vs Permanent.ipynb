{"cells": [{"cell_type": "code", "execution_count": 1, "id": "9e74720b-36c3-4d6b-96f5-c46b851c9126", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/02 03:09:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql import SparkSession\n\n# Initialize SparkSession with Hive support\nspark = SparkSession.builder \\\n    .appName(\"TableDemo\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# Read CSV from HDFS\nhdfs_path = \"/tmp/customers_100.csv\"  # Update path as per your HDFS location\ndf = spark.read.option(\"header\", True).csv(hdfs_path)\n\n# Show Data\ndf.show(5)\n"}, {"cell_type": "code", "execution_count": 18, "id": "49e3be9a-d909-4b6b-bb55-754133c023c1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------+-----------+\n|namespace|tableName     |isTemporary|\n+---------+--------------+-----------+\n|         |temp_customers|true       |\n+---------+--------------+-----------+\n\n"}], "source": "spark.sql('show tables').show(truncate=False)"}, {"cell_type": "code", "execution_count": 13, "id": "2dd376f4-3cbd-4cd2-9763-4cb48d862deb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('drop table temp_customers_2').show()"}, {"cell_type": "code", "execution_count": 15, "id": "b9d1d874-eb52-43a8-b315-0262df214f30", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "### Querying Temporary Table (Exists only in this session) ###\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "\n# ---------------- Step 1: Create a Temporary Table (Session-based) ---------------- #\ndf.createOrReplaceTempView(\"temp_customers\")\n\nprint(\"### Querying Temporary Table (Exists only in this session) ###\")\nspark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()\n"}, {"cell_type": "code", "execution_count": 17, "id": "00ce743e-467f-4f77-a21b-61c932bbc97a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "### Querying Global Temporary Table ###\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "\n# ---------------- Step 2: Create a Global Temporary Table (Accessible across sessions) ---------------- #\ndf.createOrReplaceGlobalTempView(\"global_customers\")\n\nprint(\"### Querying Global Temporary Table ###\")\nspark.sql(\"SELECT * FROM global_temp.global_customers LIMIT 5\").show()\n"}, {"cell_type": "code", "execution_count": 19, "id": "9ce38b4e-4cce-41e9-ad83-cace9ac6db05", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from global_temp.global_customers limit 5').show()"}, {"cell_type": "code", "execution_count": 20, "id": "8efacdf3-c44f-4169-b6d0-c14559c26419", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/02 03:16:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}, {"name": "stdout", "output_type": "stream", "text": "### Querying Persistent Table (Accessible across sessions and applications) ###\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|customer_id|      name|     city|      state|country|registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3|Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n+-----------+----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "\n# ---------------- Step 3: Create a Persistent Table (Stored in Hive Metastore) ---------------- #\nspark.sql(\"DROP TABLE IF EXISTS customers_persistent\")\ndf.write.mode(\"overwrite\").saveAsTable(\"customers_persistent\")\n\nprint(\"### Querying Persistent Table (Accessible across sessions and applications) ###\")\nspark.sql(\"SELECT * FROM customers_persistent LIMIT 5\").show()\n"}, {"cell_type": "code", "execution_count": 23, "id": "b378096c-4ca1-4f17-b97f-628a07ac46c4", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+------------------------------------------------------------+-------+\n|col_name                    |data_type                                                   |comment|\n+----------------------------+------------------------------------------------------------+-------+\n|customer_id                 |string                                                      |null   |\n|name                        |string                                                      |null   |\n|city                        |string                                                      |null   |\n|state                       |string                                                      |null   |\n|country                     |string                                                      |null   |\n|registration_date           |string                                                      |null   |\n|is_active                   |string                                                      |null   |\n|                            |                                                            |       |\n|# Detailed Table Information|                                                            |       |\n|Database                    |default                                                     |       |\n|Table                       |customers_persistent                                        |       |\n|Owner                       |root                                                        |       |\n|Created Time                |Sun Feb 02 03:16:06 UTC 2025                                |       |\n|Last Access                 |UNKNOWN                                                     |       |\n|Created By                  |Spark 3.3.2                                                 |       |\n|Type                        |MANAGED                                                     |       |\n|Provider                    |parquet                                                     |       |\n|Statistics                  |3898 bytes                                                  |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_persistent|       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe |       |\n+----------------------------+------------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended customers_persistent').show(truncate =False)"}, {"cell_type": "code", "execution_count": 28, "id": "c705311b-8554-4e0b-900c-06f9669603e0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "### Querying Global Temporary Table ###\n+-----------+----------------+-----------+\n|  namespace|       tableName|isTemporary|\n+-----------+----------------+-----------+\n|global_temp|global_customers|       true|\n+-----------+----------------+-----------+\n\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "# Create a new session within the same Spark application\nspark_new = spark.newSession()\n\n# Verify the view exists\nprint(\"### Querying Global Temporary Table ###\")\nspark_new.sql(\"SHOW TABLES IN global_temp\").show()\n\n# Query the Global Temp View\nspark_new.sql(\"SELECT * FROM global_temp.global_customers\").show()\n"}, {"cell_type": "code", "execution_count": 29, "id": "4c1ea689-ac2e-4278-ba10-460f3109e1aa", "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "Table or view not found: temp_customers; line 1 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [temp_customers], [], false\n", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_9027/193810830.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM temp_customers LIMIT 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: temp_customers; line 1 pos 14;\n'GlobalLimit 5\n+- 'LocalLimit 5\n   +- 'Project [*]\n      +- 'UnresolvedRelation [temp_customers], [], false\n"]}], "source": "spark_new.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"}, {"cell_type": "code", "execution_count": 32, "id": "593764bf-20f1-4509-ad71-c0d76d8107a1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|customers_persistent|      false|\n|         |      temp_customers|       true|\n+---------+--------------------+-----------+\n\n"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 33, "id": "0f2862a2-ee6c-49f9-aa9c-cc727aa8b1d1", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}