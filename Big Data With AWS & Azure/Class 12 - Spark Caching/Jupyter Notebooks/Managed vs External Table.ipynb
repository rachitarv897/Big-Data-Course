{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# \ud83d\udccc Managed vs External Tables in Spark\n\nThis notebook explains **Managed vs External Tables in Spark** using an example dataset (`customers_1mb.csv`). It also demonstrates how to configure the **default warehouse path** and verify tables using **Hive Metastore**.\n\n## \ud83d\udd39 Key Concepts\n- **Managed Table**: Spark fully controls the **data and metadata**.\n- **External Table**: Spark manages only **metadata**, while the data remains **outside** Spark's control.\n- **Hive Metastore**: Stores **table definitions** to enable SQL-like querying in Spark.\n\n---"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/02 03:33:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "# \u2705 Step 1: Import necessary libraries\nfrom pyspark.sql import SparkSession\n\n\n# \u2705 Step 3: Create Spark session with Hive support\nspark = SparkSession.builder \\\n    .appName('ManagedVsExternalTables') \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n# print(f\"Spark session created with warehouse location: {warehouse_location}\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd0d Checking Current Warehouse Directory\nYou can verify the current **Spark SQL warehouse directory** using the command below."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": "'file:/spark-warehouse'"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "# Show the configured warehouse directory\nspark.conf.get('spark.sql.warehouse.dir')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcc2 Loading the Dataset\nNow, we load the **customers_1mb.csv** dataset."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n |-- registration_date: timestamp (nullable = true)\n |-- is_active: boolean (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Load CSV data into a DataFrame\ndf = spark.read \\\n    .format('csv') \\\n    .option('header', 'True') \\\n    .option('inferSchema', 'True') \\\n    .load('/tmp/customers_100.csv')\n\n# Display DataFrame schema\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd04 Creating a Temporary View\nWe'll create a **temporary view** to allow SQL-like queries before creating tables."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------+---------+-----------+-------+-------------------+---------+\n|customer_id|      name|     city|      state|country|  registration_date|is_active|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n|          0|Customer_0|     Pune|Maharashtra|  India|2023-06-29 00:00:00|    false|\n|          1|Customer_1|Bangalore| Tamil Nadu|  India|2023-12-07 00:00:00|     true|\n|          2|Customer_2|Hyderabad|    Gujarat|  India|2023-10-27 00:00:00|     true|\n|          3|Customer_3|Bangalore|  Karnataka|  India|2023-10-17 00:00:00|    false|\n|          4|Customer_4|Ahmedabad|  Karnataka|  India|2023-03-14 00:00:00|    false|\n+-----------+----------+---------+-----------+-------+-------------------+---------+\n\n"}], "source": "# Create a temporary view for querying\ndf.createOrReplaceTempView(\"temp_customers\")\n\n# Query the view\nspark.sql(\"SELECT * FROM temp_customers LIMIT 5\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83c\udfd7 Creating a **Managed Table**\n- Spark **stores** the data inside the **warehouse directory** (`/tmp/mera/warehouse`).\n- If you **drop** this table, the **data is also deleted**."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n25/02/02 03:35:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n25/02/02 03:35:50 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "\u2705 Managed table 'managed_customers' created.\n"}], "source": "# Creating a Managed Table\nspark.sql(\"DROP TABLE IF EXISTS managed_customers\")\nspark.sql(\"\"\"\n    CREATE TABLE managed_customers AS \n    SELECT * FROM temp_customers\n\"\"\")\nprint(\"\u2705 Managed table 'managed_customers' created.\")"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+---------------------------------------------------------+-------+\n|col_name                    |data_type                                                |comment|\n+----------------------------+---------------------------------------------------------+-------+\n|customer_id                 |int                                                      |null   |\n|name                        |string                                                   |null   |\n|city                        |string                                                   |null   |\n|state                       |string                                                   |null   |\n|country                     |string                                                   |null   |\n|registration_date           |timestamp                                                |null   |\n|is_active                   |boolean                                                  |null   |\n|                            |                                                         |       |\n|# Detailed Table Information|                                                         |       |\n|Database                    |default                                                  |       |\n|Table                       |managed_customers                                        |       |\n|Owner                       |root                                                     |       |\n|Created Time                |Sun Feb 02 03:35:50 UTC 2025                             |       |\n|Last Access                 |UNKNOWN                                                  |       |\n|Created By                  |Spark 3.3.2                                              |       |\n|Type                        |MANAGED                                                  |       |\n|Provider                    |hive                                                     |       |\n|Table Properties            |[transient_lastDdlTime=1738467352]                       |       |\n|Statistics                  |6315 bytes                                               |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/managed_customers|       |\n+----------------------------+---------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended managed_customers').show(truncate=False)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\n-rwxr-xr-x   2 root hadoop       6315 2025-02-02 03:35 /user/hive/warehouse/managed_customers/part-00000-695ee9bd-dbce-4dda-8f90-c66e35b19d20-c000\n"}], "source": "!hdfs dfs -ls /user/hive/warehouse/managed_customers"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcc2 Creating an **External Table**\n- The data **remains** in `/tmp/customers_1mb.csv`.\n- If you **drop** this table, the **data is not deleted**."}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u2705 External table 'external_customers' created.\n"}, {"name": "stderr", "output_type": "stream", "text": "25/02/02 03:41:16 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider CSV. Persisting data source table `default`.`external_customers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"}], "source": "# Creating an External Table\nspark.sql(\"DROP TABLE IF EXISTS external_customers\")\nspark.sql(\"\"\"\n    CREATE EXTERNAL TABLE external_customers \n    USING CSV \n    LOCATION '/tmp/customers_1mb.csv'\n\"\"\")\nprint(\"\u2705 External table 'external_customers' created.\")"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------+-------+\n|col_name                    |data_type                                         |comment|\n+----------------------------+--------------------------------------------------+-------+\n|_c0                         |string                                            |null   |\n|_c1                         |string                                            |null   |\n|_c2                         |string                                            |null   |\n|_c3                         |string                                            |null   |\n|_c4                         |string                                            |null   |\n|_c5                         |string                                            |null   |\n|_c6                         |string                                            |null   |\n|                            |                                                  |       |\n|# Detailed Table Information|                                                  |       |\n|Database                    |default                                           |       |\n|Table                       |external_customers                                |       |\n|Owner                       |root                                              |       |\n|Created Time                |Sun Feb 02 03:41:16 UTC 2025                      |       |\n|Last Access                 |UNKNOWN                                           |       |\n|Created By                  |Spark 3.3.2                                       |       |\n|Type                        |EXTERNAL                                          |       |\n|Provider                    |CSV                                               |       |\n|Location                    |hdfs://my-cluster-m/tmp/customers_1mb.csv         |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat  |       |\n+----------------------------+--------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended external_customers').show(truncate=False)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\udd0d Verify External Table\nRun the following command to check its location:\n```bash\n!hdfs dfs -ls /tmp/customers_1mb.csv\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "---\n## \ud83d\udcca Verifying Tables in Hive Metastore\nYou can check all available tables in Spark using:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|customers_persistent|      false|\n|  default|  external_customers|      false|\n|  default|   managed_customers|      false|\n|         |      temp_customers|       true|\n+---------+--------------------+-----------+\n\n"}], "source": "# Show tables in Spark\nspark.sql(\"SHOW TABLES\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### \ud83d\uded1 Dropping the Tables\nDropping a **Managed Table** deletes both **metadata and data**, while dropping an **External Table** deletes only **metadata**."}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": " Tables dropped successfully.\n"}], "source": "# Drop managed table (Data is deleted!)\nspark.sql(\"DROP TABLE IF EXISTS managed_customers\")\n\n# Drop external table (Data is NOT deleted!)\nspark.sql(\"DROP TABLE IF EXISTS external_customers\")\n\nprint(\" Tables dropped successfully.\")"}, {"cell_type": "markdown", "metadata": {}, "source": "##  Summary\n| Feature | Managed Table | External Table |\n|---------|--------------|----------------|\n| Data Location | Inside warehouse | Custom location |\n| Dropping Table | Deletes data | Only deletes metadata |\n| Performance | Optimized by Spark | Depends on external storage |\n\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Why Spark uses Hive Metastore for Sceh"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "spark.stop()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hadoop fs -mkdir /tmp/mera"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "mkdir: `/tmp/mera/warehouse': File exists\n"}], "source": "!hadoop fs -mkdir /tmp/mera/warehouse"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"ename": "AttributeError", "evalue": "'SparkSession' object has no attribute 'config'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_21247/3499198947.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# \u2705 Step 3: Create Spark session with Hive support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mspark_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.sql.warehouse.dir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarehouse_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mnewSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'config'"]}], "source": "# \u2705 Step 1: Import necessary libraries\nfrom pyspark.sql import SparkSession\n\n\nwarehouse_location = '/tmp/mera/warehouse'\n\n# \u2705 Step 3: Create Spark session with Hive support\nspark_self = spark \\\n    .config('spark.sql.warehouse.dir', warehouse_location)\\\n    .enableHiveSupport() \\\n    .newSession()\n\n# print(f\"Spark session created with warehouse location: {warehouse_location}\")"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": "'file:/spark-warehouse'"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "# Show the configured warehouse directory\nspark.conf.get('spark.sql.warehouse.dir')"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}