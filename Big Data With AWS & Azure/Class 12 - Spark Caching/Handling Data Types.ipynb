{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# PySpark Basics: Handling Different Data Types\nThis notebook is designed for beginners to learn the basics of PySpark, focusing on handling different data types (integer, string, float, and date). We'll also add more date columns to demonstrate how different date formats are handled."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 1: Set Up PySpark\nBefore we start, we need to install and set up PySpark in the notebook."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/02 02:44:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "\n\n# Import SparkSession\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"PySpark Basics\") \\\n    .getOrCreate()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `SparkSession`: This is the entry point to use PySpark. It allows us to create DataFrames and interact with Spark.\n- `builder`: Used to configure the Spark session.\n- `appName`: Sets a name for the Spark application.\n- `getOrCreate()`: Creates a new Spark session or reuses an existing one."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 2: Create a DataFrame\nLet's create a DataFrame from the provided data. A DataFrame is a distributed collection of data organized into named columns."}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:===================>                                       (1 + 2) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---+------------+---------+-----------+------+---------+\n| id|        name|     city|       date|amount|is_active|\n+---+------------+---------+-----------+------+---------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|\n|  2|  Jane Smith|    Delhi| 2023-05-20| 89.50|    False|\n|  3|Robert Brown|   Mumbai|InvalidDate|200.00|     True|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|\n|  6|  Sarah Blue|Hyderabad|InvalidDate|300.40|       No|\n+---+------------+---------+-----------+------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Sample data\ndata = [\n    (1, \"John Doe\", \"Bangalore\", \"2023-01-15\", \"152.75\", \"True\"),\n    (2, \"Jane Smith\", \"Delhi\", \"2023-05-20\", \"89.50\", \"False\"),\n    (3, \"Robert Brown\", \"Mumbai\", \"InvalidDate\", \"200.00\", \"True\"),\n    (4, \"Linda White\", \"Kolkata\", \"2023-02-29\", None, \"yes\"),  # Feb 29 invalid in 2023\n    (5, \"Mike Green\", \"Chennai\", \"2023-08-10\", \"NaN\", \"1\"),  # NaN needs handling\n    (6, \"Sarah Blue\", \"Hyderabad\", \"InvalidDate\", \"300.40\", \"No\")\n]\n\n# Define column names\ncolumns = [\"id\", \"name\", \"city\", \"date\", \"amount\", \"is_active\"]\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema=columns)\n\n# Show the DataFrame\ndf.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `data`: This is the raw data in the form of a list of tuples.\n- `columns`: This is a list of column names for the DataFrame.\n- `createDataFrame`: This function creates a DataFrame from the data and column names.\n- `show()`: Displays the first 20 rows of the DataFrame."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 3: Explore Schema and Data Types\nLet's check the schema of the DataFrame to understand the default data types assigned by PySpark."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- date: string (nullable = true)\n |-- amount: string (nullable = true)\n |-- is_active: string (nullable = true)\n\n"}], "source": "# Print the schema\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `printSchema()`: Displays the schema of the DataFrame, including column names and data types.\n- By default, PySpark infers the data types. For example, `id` is inferred as `integer`, `name` as `string`, and `date` as `string`."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 4: Handle Integer Column (`id`)\nLet's perform basic operations on the `id` column (integer type)."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'id'>"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "df.id"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'id'>"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "df['id']"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----------+---------+-----------+------+---------+\n| id|       name|     city|       date|amount|is_active|\n+---+-----------+---------+-----------+------+---------+\n|  4|Linda White|  Kolkata| 2023-02-29|  null|      yes|\n|  5| Mike Green|  Chennai| 2023-08-10|   NaN|        1|\n|  6| Sarah Blue|Hyderabad|InvalidDate|300.40|       No|\n+---+-----------+---------+-----------+------+---------+\n\n+---+------------+---------+-----------+------+---------+---------+\n| id|        name|     city|       date|amount|is_active|id_double|\n+---+------------+---------+-----------+------+---------+---------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|        2|\n|  2|  Jane Smith|    Delhi| 2023-05-20| 89.50|    False|        4|\n|  3|Robert Brown|   Mumbai|InvalidDate|200.00|     True|        6|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|        8|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|       10|\n|  6|  Sarah Blue|Hyderabad|InvalidDate|300.40|       No|       12|\n+---+------------+---------+-----------+------+---------+---------+\n\n"}], "source": "# Filter rows where id > 3\ndf.filter(df.id > 3).show()\n\n# Add a new column with id multiplied by 2\ndf = df.withColumn(\"id_double\", df.id * 2)\ndf.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `filter()`: Filters rows based on a condition. Here, we filter rows where `id > 3`.\n- `withColumn()`: Adds a new column or replaces an existing column. Here, we add a new column `id_double` where each value is `id * 2`."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 5: Handle String Column (`name` and `city`)\nLet's perform basic operations on string columns."}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import *"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------------+---------+-----------+------+---------+---------+------------+\n| id|        name|     city|       date|amount|is_active|id_double|  name_upper|\n+---+------------+---------+-----------+------+---------+---------+------------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|        2|    JOHN DOE|\n|  2|  Jane Smith|    Delhi| 2023-05-20| 89.50|    False|        4|  JANE SMITH|\n|  3|Robert Brown|   Mumbai|InvalidDate|200.00|     True|        6|ROBERT BROWN|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|        8| LINDA WHITE|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|       10|  MIKE GREEN|\n|  6|  Sarah Blue|Hyderabad|InvalidDate|300.40|       No|       12|  SARAH BLUE|\n+---+------------+---------+-----------+------+---------+---------+------------+\n\n+---+--------+---------+----------+------+---------+---------+----------+\n| id|    name|     city|      date|amount|is_active|id_double|name_upper|\n+---+--------+---------+----------+------+---------+---------+----------+\n|  1|John Doe|Bangalore|2023-01-15|152.75|     True|        2|  JOHN DOE|\n+---+--------+---------+----------+------+---------+---------+----------+\n\n"}], "source": "# Convert name to uppercase\ndf = df.withColumn(\"name_upper\", upper(df.name))\ndf.show()\n\n# Filter rows where city starts with 'B'\ndf.filter(df.city.startswith(\"B\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `upper()`: Converts a string column to uppercase.\n- `startswith()`: Filters rows where the string column starts with a specific value."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 6: Handle Float Column (`amount`)\nLet's handle the `amount` column (float type)."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------------+---------+-----------+------+---------+---------+------------+\n| id|        name|     city|       date|amount|is_active|id_double|  name_upper|\n+---+------------+---------+-----------+------+---------+---------+------------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|        2|    JOHN DOE|\n|  2|  Jane Smith|    Delhi| 2023-05-20|  89.5|    False|        4|  JANE SMITH|\n|  3|Robert Brown|   Mumbai|InvalidDate| 200.0|     True|        6|ROBERT BROWN|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|        8| LINDA WHITE|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|       10|  MIKE GREEN|\n|  6|  Sarah Blue|Hyderabad|InvalidDate| 300.4|       No|       12|  SARAH BLUE|\n+---+------------+---------+-----------+------+---------+---------+------------+\n\n"}], "source": "# Replace 'NaN' with null and cast to float\nfrom pyspark.sql.functions import col\ndf = df.withColumn(\"amount\", col(\"amount\").cast(\"float\"))\ndf.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `cast()`: Converts a column to a specific data type. Here, we convert `amount` to `float`.\n- `selectExpr()`: Allows us to run SQL expressions. Here, we calculate the average of the `amount` column."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 7: Handle Date Column (`date`)\nLet's handle the `date` column, including invalid dates. We'll also add more date columns to demonstrate different date formats."}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+\n| id|        name|     city|       date|amount|is_active|id_double|  name_upper|date_alt_format|date_with_time|parsed_date|\n+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|        2|    JOHN DOE|     2023-01-15|    2023-01-15| 2023-01-15|\n|  2|  Jane Smith|    Delhi| 2023-05-20|  89.5|    False|        4|  JANE SMITH|     2023-05-20|    2023-05-20| 2023-05-20|\n|  3|Robert Brown|   Mumbai|InvalidDate| 200.0|     True|        6|ROBERT BROWN|           null|          null|       null|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|        8| LINDA WHITE|           null|          null|       null|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|       10|  MIKE GREEN|     2023-08-10|    2023-08-10| 2023-08-10|\n|  6|  Sarah Blue|Hyderabad|InvalidDate| 300.4|       No|       12|  SARAH BLUE|           null|          null|       null|\n+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+\n\n+---+----------+---------+----------+------+---------+---------+----------+---------------+--------------+-----------+\n| id|      name|     city|      date|amount|is_active|id_double|name_upper|date_alt_format|date_with_time|parsed_date|\n+---+----------+---------+----------+------+---------+---------+----------+---------------+--------------+-----------+\n|  1|  John Doe|Bangalore|2023-01-15|152.75|     True|        2|  JOHN DOE|     2023-01-15|    2023-01-15| 2023-01-15|\n|  2|Jane Smith|    Delhi|2023-05-20|  89.5|    False|        4|JANE SMITH|     2023-05-20|    2023-05-20| 2023-05-20|\n|  5|Mike Green|  Chennai|2023-08-10|   NaN|        1|       10|MIKE GREEN|     2023-08-10|    2023-08-10| 2023-08-10|\n+---+----------+---------+----------+------+---------+---------+----------+---------------+--------------+-----------+\n\n"}], "source": "from pyspark.sql.functions import to_date\n\n# Add more date columns with different formats\ndf = df.withColumn(\"date_alt_format\", to_date(df.date, \"dd-MM-yyyy\"))  # Alternate format\ndf = df.withColumn(\"date_with_time\", to_date(df.date, \"yyyy-MM-dd HH:mm:ss\"))  # Date with time\n\n# Convert valid dates and handle invalid dates\ndf = df.withColumn(\"parsed_date\", to_date(df.date, \"yyyy-MM-dd\"))\n\ndf = df.withColumn(\"date_alt_format\", to_date(df.parsed_date, \"dd-MM-yyyy\"))  # Alternate format\ndf = df.withColumn(\"date_with_time\", to_date(df.parsed_date, \"yyyy-MM-dd HH:mm:ss\"))  # Date with time\n\n\n# Show the DataFrame\ndf.show()\n\n# Filter rows with valid dates\ndf.filter(df.parsed_date.isNotNull()).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `to_date()`: Converts a string column to a date column using the specified format.\n- `isNotNull()`: Filters rows where the column is not null."}, {"cell_type": "markdown", "metadata": {}, "source": "## Step 8: Add 3 More Columns\nLet's add 3 more columns to the DataFrame to showcase additional operations."}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+--------------+------------+--------------+\n| id|        name|     city|       date|amount|is_active|id_double|  name_upper|date_alt_format|date_with_time|parsed_date|is_active_bool|constant_col|amount_plus_10|\n+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+--------------+------------+--------------+\n|  1|    John Doe|Bangalore| 2023-01-15|152.75|     True|        2|    JOHN DOE|     2023-01-15|    2023-01-15| 2023-01-15|          true|     PySpark|        162.75|\n|  2|  Jane Smith|    Delhi| 2023-05-20|  89.5|    False|        4|  JANE SMITH|     2023-05-20|    2023-05-20| 2023-05-20|         false|     PySpark|          99.5|\n|  3|Robert Brown|   Mumbai|InvalidDate| 200.0|     True|        6|ROBERT BROWN|           null|          null|       null|          true|     PySpark|         210.0|\n|  4| Linda White|  Kolkata| 2023-02-29|  null|      yes|        8| LINDA WHITE|           null|          null|       null|          true|     PySpark|          null|\n|  5|  Mike Green|  Chennai| 2023-08-10|   NaN|        1|       10|  MIKE GREEN|     2023-08-10|    2023-08-10| 2023-08-10|          true|     PySpark|           NaN|\n|  6|  Sarah Blue|Hyderabad|InvalidDate| 300.4|       No|       12|  SARAH BLUE|           null|          null|       null|         false|     PySpark|         310.4|\n+---+------------+---------+-----------+------+---------+---------+------------+---------------+--------------+-----------+--------------+------------+--------------+\n\n"}], "source": "from pyspark.sql.functions import lit, when\n\n# Add a boolean column based on `is_active`\ndf = df.withColumn(\"is_active_bool\", when(df.is_active.isin(\"True\", \"yes\", \"1\"), True).otherwise(False))\n\n# Add a constant column\ndf = df.withColumn(\"constant_col\", lit(\"PySpark\"))\n\n# Add a calculated column (amount + 10)\ndf = df.withColumn(\"amount_plus_10\", df.amount + 10)\n\n# Show the final DataFrame\ndf.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explanation:\n- `when()`: Used for conditional logic. Here, we convert `is_active` to a boolean column.\n- `lit()`: Adds a constant value to a column.\n- `amount + 10`: Performs arithmetic operations on a column."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}