{"cells": [{"cell_type": "markdown", "id": "618e7a7f-5e96-43ec-8ed5-fcb76fab9b12", "metadata": {}, "source": "# Caching In Spark Table"}, {"cell_type": "code", "execution_count": 1, "id": "e284c85d-245c-449f-9d20-47012f2042c4", "metadata": {}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "374ed9dc-f6c1-484e-bbc2-693aab1aac5f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/02/08 03:21:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n.appName('Spark Table Caching')\\\n.enableHiveSupport()\\\n.getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "5389358b-9234-40b1-a996-69c58bbbc0b5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n[Stage 1:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+--------------------+-----------+\n|namespace|           tableName|isTemporary|\n+---------+--------------------+-----------+\n|  default|       customers_100|      false|\n|  default|     customers_500mb|      false|\n|  default|external_customers_2|      false|\n+---------+--------------------+-----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 6, "id": "6feffcfd-5695-4164-802c-71c89bc02b12", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.format('csv').option('header','true').load('/tmp/customers_100.csv')"}, {"cell_type": "code", "execution_count": 7, "id": "69c1f855-a7b2-4887-9d55-3f74a1167865", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": null, "id": "d28247ab-19c6-4abd-a831-9d5cbbb02ff3", "metadata": {}, "outputs": [], "source": "df.write.format('csv').saveAsTable('default.customers_100')"}, {"cell_type": "code", "execution_count": null, "id": "b807354a-e32b-40d8-9303-ca381702e39e", "metadata": {}, "outputs": [], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": null, "id": "9e3a1280-670b-4e8e-90f5-0352981339a7", "metadata": {}, "outputs": [], "source": "spark.sql('describe extended customers_100').show(truncate=False)"}, {"cell_type": "code", "execution_count": null, "id": "bf255c61-cc3b-4de8-9ca9-aa07eae35345", "metadata": {}, "outputs": [], "source": "!hadoop fs -ls /user/hive/warehouse/customers_100"}, {"cell_type": "code", "execution_count": null, "id": "279a360b-94c7-4e36-8389-e0beb14f24b8", "metadata": {}, "outputs": [], "source": "spark.sql('select * from customers_100 limit 5').show()"}, {"cell_type": "code", "execution_count": null, "id": "f8a99528-4c87-4c1b-987b-cbd7a1313bf9", "metadata": {}, "outputs": [], "source": "spark.sql('describe customers_100').show()"}, {"cell_type": "code", "execution_count": null, "id": "6ee030ee-4345-4f88-84f4-3539a3a639fc", "metadata": {}, "outputs": [], "source": "spark.sql('select * from customers_100 limit 5').show()"}, {"cell_type": "code", "execution_count": null, "id": "6404d798-d039-453c-ba4d-cd71c3e8142d", "metadata": {}, "outputs": [], "source": "spark.sql('cache table customers_100')"}, {"cell_type": "code", "execution_count": null, "id": "23625698-8974-4a31-bfbe-6684444aba5d", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "id": "054aca52-629d-4673-b1a5-cadce0651826", "metadata": {}, "outputs": [], "source": "spark.sql('select * from customers_100 limit 5').show()"}, {"cell_type": "code", "execution_count": null, "id": "16572e41-fddf-4353-9b7b-c617d2c5acc2", "metadata": {}, "outputs": [], "source": "spark.sql('show tables').show()"}, {"cell_type": "code", "execution_count": 5, "id": "a12b34d1-1323-4b9d-a376-169de1d237b6", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+-------------------------------------------------------+-------+\n|col_name                    |data_type                                              |comment|\n+----------------------------+-------------------------------------------------------+-------+\n|customers_id                |int                                                    |null   |\n|name                        |string                                                 |null   |\n|city                        |string                                                 |null   |\n|state                       |string                                                 |null   |\n|country                     |string                                                 |null   |\n|registration_date           |string                                                 |null   |\n|is_active                   |boolean                                                |null   |\n|                            |                                                       |       |\n|# Detailed Table Information|                                                       |       |\n|Database                    |default                                                |       |\n|Table                       |customers_500mb                                        |       |\n|Owner                       |root                                                   |       |\n|Created Time                |Thu Feb 06 11:40:32 UTC 2025                           |       |\n|Last Access                 |UNKNOWN                                                |       |\n|Created By                  |Spark 3.3.2                                            |       |\n|Type                        |MANAGED                                                |       |\n|Provider                    |csv                                                    |       |\n|Statistics                  |570783941 bytes                                        |       |\n|Location                    |hdfs://my-cluster-m/user/hive/warehouse/customers_500mb|       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe     |       |\n+----------------------------+-------------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended customers_500mb').show(truncate = False)"}, {"cell_type": "code", "execution_count": 8, "id": "05d4d10c-de21-4364-bd5f-16ff4ecf4194", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 4:=======================>                                   (2 + 2) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+-----------+---------+-----------+-------+-----------------+---------+\n|customers_id|       name|     city|      state|country|registration_date|is_active|\n+------------+-----------+---------+-----------+-------+-----------------+---------+\n|           3| Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n|           6| Customer_6|Hyderabad| Tamil Nadu|  India|       2023-07-17|    false|\n|           7| Customer_7|Hyderabad| Tamil Nadu|  India|       2023-08-18|     true|\n|          20|Customer_20|Hyderabad| Tamil Nadu|  India|       2023-02-19|     true|\n|          26|Customer_26|Hyderabad|Maharashtra|  India|       2023-12-13|     true|\n+------------+-----------+---------+-----------+-------+-----------------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select * from customers_500mb where city =\"Hyderabad\" limit 5').show()"}, {"cell_type": "code", "execution_count": 9, "id": "76fe24fd-2864-4fe4-a3bb-704caedd1792", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "DataFrame[]"}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('cache table customers_500mb')"}, {"cell_type": "code", "execution_count": 10, "id": "45adb093-12b3-46bd-8fbf-2c4b4880fb9a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+-----------+---------+-----------+-------+-----------------+---------+\n|customers_id|       name|     city|      state|country|registration_date|is_active|\n+------------+-----------+---------+-----------+-------+-----------------+---------+\n|           3| Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n|           6| Customer_6|Hyderabad| Tamil Nadu|  India|       2023-07-17|    false|\n|           7| Customer_7|Hyderabad| Tamil Nadu|  India|       2023-08-18|     true|\n|          20|Customer_20|Hyderabad| Tamil Nadu|  India|       2023-02-19|     true|\n|          26|Customer_26|Hyderabad|Maharashtra|  India|       2023-12-13|     true|\n+------------+-----------+---------+-----------+-------+-----------------+---------+\n\n"}], "source": "spark.sql('select * from customers_500mb where city =\"Hyderabad\" limit 5').show()"}, {"cell_type": "code", "execution_count": 11, "id": "9a3f0440-656e-4ad0-a0f2-6c8fe54c750d", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('uncache table customers_500mb')"}, {"cell_type": "code", "execution_count": null, "id": "cbc19d60-68dc-4ccb-895c-0bbcd279c6b4", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 12, "id": "3c026cfe-e333-4332-ac66-4229fe97479f", "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql('cache lazy table customers_500mb')"}, {"cell_type": "code", "execution_count": 13, "id": "c6af04dd-1e9f-4f8c-8521-86f84fe0cdcf", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 15:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+----------+---------+-----------+-------+-----------------+---------+\n|customers_id|      name|     city|      state|country|registration_date|is_active|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n|        null|      name|     city|      state|country|registration_date|     null|\n|           0|Customer_0|   Mumbai|  Telangana|  India|       2023-03-21|     true|\n|           1|Customer_1|  Chennai|West Bengal|  India|       2023-05-27|    false|\n|           2|Customer_2|     Pune|  Karnataka|  India|       2023-10-11|    false|\n|           3|Customer_3|Hyderabad|    Gujarat|  India|       2023-11-11|    false|\n+------------+----------+---------+-----------+-------+-----------------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select * from customers_500mb limit 5').show()"}, {"cell_type": "code", "execution_count": 15, "id": "8217c7f5-63be-42cc-ac70-728407917599", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 16:==============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+--------+\n|     city|count(1)|\n+---------+--------+\n|Bangalore| 1094195|\n|  Chennai| 1095052|\n|   Mumbai| 1095815|\n|Ahmedabad| 1097162|\n|  Kolkata| 1096777|\n|     city|       1|\n|     Pune| 1095748|\n|    Delhi| 1096183|\n|Hyderabad| 1096426|\n+---------+--------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql('select city, count (*) from customers_500mb group by city').show()"}, {"cell_type": "code", "execution_count": 17, "id": "7bc7ab8c-574f-4e9c-9a69-bfe237c4446f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Parsed Logical Plan ==\n'Aggregate ['city], ['city, unresolvedalias('count(1), None)]\n+- 'UnresolvedRelation [customers_500mb], [], false\n\n== Analyzed Logical Plan ==\ncity: string, count(1): bigint\nAggregate [city#151], [city#151, count(1) AS count(1)#1202L]\n+- SubqueryAlias spark_catalog.default.customers_500mb\n   +- Relation default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] csv\n\n== Optimized Logical Plan ==\nAggregate [city#151], [city#151, count(1) AS count(1)#1202L]\n+- Project [city#151]\n   +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n         +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[city#151], functions=[count(1)], output=[city#151, count(1)#1202L])\n   +- Exchange hashpartitioning(city#151, 200), ENSURE_REQUIREMENTS, [plan_id=325]\n      +- HashAggregate(keys=[city#151], functions=[partial_count(1)], output=[city#151, count#1311L])\n         +- Scan In-memory table customers_500mb [city#151]\n               +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n                     +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n\n"}], "source": "spark.sql('select city, count (*) from customers_500mb group by city').explain(mode='extended')"}, {"cell_type": "code", "execution_count": 22, "id": "4cc6d1fb-3e43-4c95-96df-d38297f414cd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+--------+\n|     city|count(1)|\n+---------+--------+\n|Hyderabad| 1096426|\n+---------+--------+\n\n"}], "source": "spark.sql('select city, count (*) from customers_500mb where city =\"Hyderabad\" group by city').show()"}, {"cell_type": "code", "execution_count": 23, "id": "6c1771a6-a954-4b82-a37e-c9fa5b02eaeb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Parsed Logical Plan ==\n'Aggregate ['city], ['city, unresolvedalias('count(1), None)]\n+- 'Filter ('city = Hyderabad)\n   +- 'UnresolvedRelation [customers_500mb], [], false\n\n== Analyzed Logical Plan ==\ncity: string, count(1): bigint\nAggregate [city#151], [city#151, count(1) AS count(1)#1614L]\n+- Filter (city#151 = Hyderabad)\n   +- SubqueryAlias spark_catalog.default.customers_500mb\n      +- Relation default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] csv\n\n== Optimized Logical Plan ==\nAggregate [city#151], [city#151, count(1) AS count(1)#1614L]\n+- Project [city#151]\n   +- Filter (isnotnull(city#151) AND (city#151 = Hyderabad))\n      +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[city#151], functions=[count(1)], output=[city#151, count(1)#1614L])\n   +- Exchange hashpartitioning(city#151, 200), ENSURE_REQUIREMENTS, [plan_id=403]\n      +- HashAggregate(keys=[city#151], functions=[partial_count(1)], output=[city#151, count#1723L])\n         +- Filter (isnotnull(city#151) AND (city#151 = Hyderabad))\n            +- Scan In-memory table customers_500mb [city#151], [isnotnull(city#151), (city#151 = Hyderabad)]\n                  +- InMemoryRelation [customers_id#149, name#150, city#151, state#152, country#153, registration_date#154, is_active#155], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- FileScan csv default.customers_500mb[customers_id#149,name#150,city#151,state#152,country#153,registration_date#154,is_active#155] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://my-cluster-m/user/hive/warehouse/customers_500mb], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<customers_id:int,name:string,city:string,state:string,country:string,registration_date:str...\n\n"}], "source": "spark.sql('select city, count (*) from customers_500mb where city =\"Hyderabad\" group by city').explain(mode='extended')"}, {"cell_type": "code", "execution_count": 25, "id": "74380b1a-5e68-4779-8f49-3e32496a0f41", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------+-------+\n|col_name                    |data_type                                         |comment|\n+----------------------------+--------------------------------------------------+-------+\n|customer_id                 |int                                               |null   |\n|name                        |string                                            |null   |\n|city                        |string                                            |null   |\n|state                       |string                                            |null   |\n|country                     |string                                            |null   |\n|registration_date           |string                                            |null   |\n|is_active                   |boolean                                           |null   |\n|                            |                                                  |       |\n|# Detailed Table Information|                                                  |       |\n|Database                    |default                                           |       |\n|Table                       |external_customers_2                              |       |\n|Owner                       |root                                              |       |\n|Created Time                |Wed Feb 05 11:55:14 UTC 2025                      |       |\n|Last Access                 |UNKNOWN                                           |       |\n|Created By                  |Spark 3.3.2                                       |       |\n|Type                        |EXTERNAL                                          |       |\n|Provider                    |csv                                               |       |\n|Location                    |hdfs://my-cluster-m/data/external_data            |       |\n|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat  |       |\n+----------------------------+--------------------------------------------------+-------+\nonly showing top 20 rows\n\n"}], "source": "spark.sql('describe extended external_customers_2').show(truncate = False)"}, {"cell_type": "code", "execution_count": 26, "id": "5f9bf376-e2d6-48da-b553-80e1f22c3fb3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "spark.sql('cache table external_customers_2').show(truncate = False)"}, {"cell_type": "code", "execution_count": 27, "id": "34f136ff-4af7-44d0-82b4-359849117933", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|customer_id|       name|     city|      state|country|registration_date|is_active|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\n|          0| Customer_0|     Pune|Maharashtra|  India|       2023-06-29|    False|\n|          1| Customer_1|Bangalore| Tamil Nadu|  India|       2023-12-07|     True|\n|          2| Customer_2|Hyderabad|    Gujarat|  India|       2023-10-27|     True|\n|          3| Customer_3|Bangalore|  Karnataka|  India|       2023-10-17|    False|\n|          4| Customer_4|Ahmedabad|  Karnataka|  India|       2023-03-14|    False|\n|          5| Customer_5|Hyderabad|  Karnataka|  India|       2023-07-28|    False|\n|          6| Customer_6|     Pune|      Delhi|  India|       2023-08-29|    False|\n|          7| Customer_7|Ahmedabad|West Bengal|  India|       2023-12-28|     True|\n|          8| Customer_8|     Pune|  Karnataka|  India|       2023-06-22|     True|\n|          9| Customer_9|   Mumbai|  Telangana|  India|       2023-01-05|     True|\n|         10|Customer_10|     Pune|    Gujarat|  India|       2023-08-05|     True|\n|         11|Customer_11|    Delhi|West Bengal|  India|       2023-08-02|    False|\n|         12|Customer_12|  Chennai|    Gujarat|  India|       2023-11-21|    False|\n|         13|Customer_13|  Chennai|  Karnataka|  India|       2023-11-06|     True|\n|         14|Customer_14|Hyderabad| Tamil Nadu|  India|       2023-02-07|    False|\n|         15|Customer_15|   Mumbai|    Gujarat|  India|       2023-03-02|     True|\n|         16|Customer_16|  Chennai|  Karnataka|  India|       2023-04-05|    False|\n|         17|Customer_17|Hyderabad|West Bengal|  India|       2023-08-21|    False|\n|         18|Customer_18|     Pune|      Delhi|  India|       2023-10-04|     True|\n|         19|Customer_19|  Kolkata|    Gujarat|  India|       2023-02-05|     True|\n+-----------+-----------+---------+-----------+-------+-----------------+---------+\nonly showing top 20 rows\n\n"}], "source": "df.show()"}, {"cell_type": "code", "execution_count": 28, "id": "9e906142-7f89-4f9c-8e08-bd9bec790292", "metadata": {}, "outputs": [], "source": "spark.stop()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}